{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition about RMSprop and Adam optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two well-known optimizaion algorithms are widely used in the deep learning. Here I give some intuition about them.  \n",
    "First, I begin with the gradient descent with momentum. For simplicity, I only consider updating one parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient descent with momentum\n",
    "On the iteration $t$, the weight $w$ is updated, for the gradient descent with momentum, as follow:\n",
    "\\begin{align} \n",
    " v_{dw}(t) &= \\beta_1 v_{dw}(t-1) + (1-\\beta_1)dw \\\\\n",
    "w &= w - \\alpha v_{dw}(t),\n",
    "\\end{align}\n",
    "where $dw$ is the derivative of the loss $J$ at this interation with respect to the weight $w$. And $\\beta_1$ and $\\alpha$ (learning rate) are two hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm will give a very small $V_{dw}(t)$ of an oscilating $dw$ to damp out the oscillation. On the non-oscillating direction, $w$ is updated normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RMSprop (Root Mean Square prop)\n",
    "In the algorithm, on the iteration $t$, the weight $w$ is updated as follows:\n",
    "\\begin{align} \n",
    " s_{dw}(t) &= \\beta_2 s_{dw}(t-1) + (1-\\beta_2)dw^2 \\\\\n",
    "w &= w - {\\alpha \\over \\sqrt{s_{dw}(t)} } dw,\n",
    "\\end{align}\n",
    "where $dw$ and $\\alpha$ is same as before and $\\beta_2$ is a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two direction corresponding two parameter $w_1$ and $w_2$. And the direction $w_1$ is a little steep and the direction $w_2$ is a little flat. We hope that on the steep direction, $w_1$, the learing rate might be smaller to avoid the overshoot and on the falt dirction, $w_2$, the learning rate might be larger to converge faster.  \n",
    "This is implemented is RMSprop. Since $s_{dw_1} > s_{dw_2}$. \n",
    "Effectively, in RMSprop, $w_1$ would be updated by a smaller learning rate $\\alpha_1 \\equiv {\\alpha \\over \\sqrt{s_{dw_1}}}$ than that of $w_2$, $\\alpha_2 \\equiv {\\alpha \\over \\sqrt{s_{dw_2}}}$, as we wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adam optimization algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent with momentum is efficient in damping out the oscillation, but can not adjust the learning rate along different directions according to the steepness. \n",
    "On the other side, RMSprop can adjust the learning rate automatically, but not very specific in damping the oscillation. For example, if a parameter is oscillation along a plateau dirction, it would be difficult for RMSprop to damp out the oscillating. \n",
    "\n",
    "By combining both of the algorithms together, Adam optimization algorithm can damp out the oscillation and at the same time, adjust the learing rate according to the steepness!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Adam algorithm, at the iteration $t$, the weight $w$ is updated as follows:\n",
    "\\begin{align} \n",
    " v_{dw}(t) &= {1 \\over 1 - \\beta_1^t} [\\beta_1 v_{dw}(t-1) + (1-\\beta_1)dw]    \\\\\n",
    " s_{dw}(t) &= {1 \\over 1 - \\beta_2^t}[\\beta_2 s_{dw}(t-1) + (1-\\beta_2)dw^2 ] \\\\\n",
    "w &= w - {\\alpha \\over \\sqrt{s_{dw}(t)} } v_{dw}(t),\n",
    "\\end{align}\n",
    "This almost the combination of the first two algorithms, except the extra factors $1/( 1 - \\beta_1^t)$ and $1/( 1 - \\beta_2^t)$. These factors are used as the bias correction to get better exponatially weighted averages at the initial several iterations.\n",
    "\n",
    "Let me display the effection of each term:\n",
    "- $v_{dw}$: damping out the oscillation by using the exponentially weighted average of the gradient.\n",
    "- $s_{dw}$: adjuct the learning rate according to the steepness by dividing the learing rate by the root mean square of the exponentially weighted average of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
