{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, I learned XGBoost by watching a series of the amazing videos hosted by [StatQuest with Josh Starmer](https://www.youtube.com/watch?v=OtD8wVaFm6E). In this notebook, I summary what I have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How does XGBoost work\n",
    "\n",
    "XGBoost builds a model by construcing a series of decision binary trees with each trees fitted to the residual of the previous trees. The output of XGBoost is the combination of these trees. The initial tree, ususally, is just a naive guess. However, after several trees are built, XGBoost can fit the traing data very well.\n",
    "The residual is just the difference between the targets and predictions of the trees that have been constructed.\n",
    "\n",
    "Then in order understand how XGBoost works, we should understand two questions:\n",
    "1. how to split a leaf\n",
    "1. how to make a prediction.\n",
    "\n",
    "\n",
    "Since, on the surface, the other two answers are different on regression and classification,\n",
    "XGBoost will be explained in Regression and Classification respectively.\n",
    "\n",
    "**Denote the residuals of the $n$ samples in a leaf by $r_i$ with i=1,...,n.**\n",
    "\n",
    "## 1.1 Split the leaf by using Similarity and Gain\n",
    "Then we can calculate the **Similarity Score**, $S$ of the leaf.  \n",
    "For a regression, it is\n",
    "\\begin{equation}\n",
    " S = { (\\Sigma_{i=1}^n r_i)^2 \\over \\lambda + n }.\n",
    "\\end{equation}\n",
    "For a binary classification, it is\n",
    "$$\n",
    " S = { (\\Sigma_{i=1}^n r_i)^2 \\over \\lambda + \\Sigma_{j=1}^n p_j(1-p_j)}.\n",
    "$$\n",
    "Here $\\lambda$ is the regularization parameter. And $p_j$ is the prediction of the $j$th sample in the leaf\n",
    "given by the trees that have been constructed. Or in other words, $p_j$ is the prediction of the $j$th sample used \n",
    "to calculate its residual, $r_j$.\n",
    "\n",
    "Then we try to find a threshold to split the leaf into two leaves (left and right)\n",
    "and calculate the similarities scores on the two sub-leaves and denoting them as $S_l$ and $S_r$.\n",
    "Then we can calculate the **Gain** for this split:\n",
    "$$\n",
    "G = S_l + S_r - S.\n",
    "$$\n",
    "Then we can try different thresholds to split the leaf and select the threshold with the largest gain\n",
    "to construct the leaf in the tree. Repeating this procedure, we can construnct a binary tree fitted to the residuals.\n",
    "But be sure, $G$ must be positive. Otherwise the leaf would not be split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Calculate the predictin\n",
    "If this leaf is a terminal node, we can calculate the output of the leaf.\n",
    "For a regression, it is \n",
    "$$\n",
    "  O = {\\Sigma_{i=1}^n r_i \\over \\lambda + n }.\n",
    "$$\n",
    "If $\\lambda =0$, this is just the average of the residuals in this leaf.  \n",
    "For a binary classifiction, it is \n",
    "$$\n",
    "  O = { \\Sigma_{i=1}^n r_i \\over \\lambda + \\Sigma_{j=1}^n p_j(1-p_j)}.\n",
    "$$\n",
    "\n",
    "Then the prediction of the built trees is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
