{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, I learned XGBoost by watching a series of the amazing videos hosted by [StatQuest with Josh Starmer](https://www.youtube.com/watch?v=OtD8wVaFm6E). In this notebook, I summary what I have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How does XGBoost work\n",
    "\n",
    "XGBoost builds a model by construcing a series of decision binary trees with each trees fitted to the residual of the previous trees. The output of XGBoost is the combination of these trees. The initial tree, ususally, is just a naive guess. However, after several trees are built, XGBoost can fit the traing data very well.\n",
    "The residual is just the difference between the targets and predictions of the trees that have been constructed.\n",
    "\n",
    "Then in order understand how XGBoost works, we should understand two questions:\n",
    "1. how to split a leaf\n",
    "1. how to make a prediction.\n",
    "\n",
    "\n",
    "Since, on the surface, the other two answers are different on regression and classification,\n",
    "XGBoost will be explained in Regression and Classification respectively.\n",
    "\n",
    "Let us assume that we have made the initial prediction and built $k-1$ XGBoost trees. And now we are builting the $k$th XGBoost tree.\n",
    "\n",
    "## 1.1 Split the leaf by using Similarity and Gain\n",
    "**Denoting the residuals of the $n$ samples in a leaf in the $k$th tree by $r_i$ $(i=1,...,n)$,**\n",
    "we can calculate the **Similarity Score**, $S$ of the leaf.  \n",
    "For regression, it is\n",
    "\\begin{equation}\n",
    " S = \\frac{ (\\sum_{i=1}^n r_i)^2}{ \\lambda + n }.\n",
    "\\end{equation}\n",
    "For binary classification, it is\n",
    "$$\n",
    " S = { {(\\sum_{i=1}^n r_i)^2} \\over {\\lambda + \\sum_{i=1}^n p^{k-1}_i(1-p^{k-1}_i)}}.\n",
    "$$\n",
    "Here $\\lambda$ is the regularization parameter. And $p^{k-1}_i$ is the prediction of the $i$th sample in the leaf\n",
    "given by the initial prediction and the $k-1$ trees that have been constructed. Or in other words, $p^{k-1}_j$ is the prediction  used to calculate the residual of the $j$th sample, $r_j$.\n",
    "\n",
    "Then we try to find a threshold to split the leaf into two leaves (left and right)\n",
    "and calculate the similarities scores on the two sub-leaves and denoting them as $S_l$ and $S_r$.\n",
    "Then we can calculate the **Gain** for this split:\n",
    "$$\n",
    "G = S_l + S_r - S.\n",
    "$$\n",
    "Then we can try different thresholds to split the leaf and select the threshold with the largest gain\n",
    "to construct the leaf in the tree. Repeating this procedure, we can construnct a XGBoost tree fitted to the residuals.\n",
    "*I am not sure, whether $G$ must be positive in order to split a leaf*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Calculate the predictin\n",
    "If this leaf is a terminal node, we can calculate the output of the leaf.\n",
    "For regression, it is \n",
    "$$\n",
    "  Q = {{\\sum_{i=1}^n r_i} \\over {\\lambda + n }}.\n",
    "$$\n",
    "If $\\lambda =0$, this is just the average of the residuals in this leaf.  \n",
    "For binary classifiction, it is \n",
    "$$\n",
    "  Q = { {\\sum_{i=1}^n r_i} \\over {\\lambda + \\sum_{j=1}^n p_j(1-p_j)}}.\n",
    "$$\n",
    "\n",
    "By combining the outputs of all of the XGBoost trees with the initial prediction, we can get the predition of the XGBoost model.  \n",
    "For regression, it is\n",
    "$$\n",
    "  p = p_0 + \\epsilon \\times \\sum_{t=1}Q_t.\n",
    "$$\n",
    "For binary classification, it is \n",
    "$$\n",
    "  p = {1 \\over {1 + e^{-Q_T}}}\n",
    "$$\n",
    "with\n",
    "$$\n",
    "  Q_T = \\ln{p_0 \\over {1-p_0}} + \\epsilon \\times \\sum_{t=1}Q_t.\n",
    "$$\n",
    "Here, $p_0$ is the initial prediction and $\\epsilon$ is the learning rate. \n",
    "And $O_t$ ($t=1,2,...$) denotes the output of the $t$-th XGBoost tree respectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Prune a tree\n",
    "\n",
    "After a XGBoost tree is built, we prune it from bottom to root  based on the gain, $G$ and the parameter $\\gamma$.\n",
    "If $G < \\gamma$, a split will be removed. A split would not be removed even its $G$ less than $\\gamma$ \n",
    "if any of its branches is kept. \n",
    "\n",
    "It is noted that setting $\\gamma=0$ does not mean not to prune the tree, \n",
    "since in this case, the branch with negative gain would be pruned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Other parameters  \n",
    "The post, [De-Mystifying XGBoost Part II](https://towardsdatascience.com/de-mystifying-xgboost-part-ii-175252dcdbc5), \n",
    "gives the explanation about the parameters in XGBoost.\n",
    "- Cover:      \n",
    "The cover is the denominator of the similarity score minus $\\lambda$. For regression, it is\n",
    "$$\n",
    " C = n.\n",
    "$$\n",
    "For classficiation, it is \n",
    "$$\n",
    " C = \\sum_i^n p_i(1-p_i).\n",
    "$$\n",
    "XGBoot, defautly, sets the minimum value of $C_m$ being $1$ and does not allowed for a leaf to have\n",
    "the cover less than $C_m$. Defautly, this requirement has no effect for regression. \n",
    "But for classification, the defaut setting effects. So, sometimes, an appropriate $C_m$ or the parameter, **min_child_weight**, should be selected by hand. However, usually, the default value justified the split well and should not be changed.\n",
    "\n",
    "- $\\lambda$:  \n",
    "A non-zero $\\lambda$ would intend to keep the number of samples in a leaf as many as possible.\n",
    "\n",
    "- Base_score:  \n",
    "The initial output. It can be set by hand when the class distribution has a big say (bias) in the final probability. \n",
    "\n",
    "- Max Delta Step:  \n",
    "A value is used to constrain the absolute value of the score from each step (tree). Default value is zero which means no constraint.\n",
    "\n",
    "- Scale Pos Weight:  \n",
    "  For imbalanced data, the weights can be set to effect the calculation of $g$ (first derivative) and $h$ (second derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mathmatical Details\n",
    "The formulas of $S$ and $O$ are different between regression and classification. \n",
    "But, underneath the surface, they have the unified form.\n",
    "This can be shown by considering the construction of the $k$th XGBoot tree as minimizing the loss function $Loss$,\n",
    "$$\n",
    " Loss = \\sum_{i=1}^m L(y_i, p^{k-1}_i + Q_i) + {1\\over2}\\lambda Q_i^2.\n",
    "$$\n",
    "Here the subscrip $i$ denote the corresponding parameter of the $i$-th sample in the traing data,\n",
    "and $\\sum_{i=1}^m$ denotes the summation over the total $m$ training samples.\n",
    "And $y_i$ is the target and $p^{k-1}_i$ is the prediction given by the initial prediction \n",
    "and the built $k-1$ XGBoost trees, just same as above. $Q_i$ denotes the prediction of the $k$-th \n",
    "XGBoost tree which will be built. The $k$-th XGBoost tree is just the one whose predictions minimize\n",
    "$Loss$. Expanding $L(y_i, p^{k-1}_i + Q_i)$ to the second order of $Q_i$, we have\n",
    "$$\n",
    "  Loss \\simeq \\sum_{i=1}^m L(y_i, p^{k-1}_i) + \\delta L\n",
    "$$\n",
    "with \n",
    "$$\n",
    "\\delta L = \\sum_l [g_l Q_l + {1\\over2}(h_l+\\lambda) Q_l^2].\n",
    "$$\n",
    "and\n",
    "$$\n",
    "  g_l = \\sum_{i_l} \\frac{\\partial L(y, x)} {\\partial x}\\bigg |_{y=y_{i_l}, x=p^{k-1}_{i_l}}\n",
    "$$\n",
    "$$\n",
    " h_l = \\sum_{i_l} \\frac{\\partial^2 L(y,x)}{\\partial x^2}\\bigg |_{y=y_{i_l}, x=p^{k-1}_{i_l}}.\n",
    "$$\n",
    "Here the subscript $l$ denotes the corresponding parameter of the $l$-th terminal leaf,\n",
    "and $\\sum_l$ denotes the summation over all of the terminal leaves. \n",
    "The subscript $i_l$ denotes the corresponding parameter of the $i_l$-th sample in the $l$-th  terminal leaf.\n",
    "And $\\sum_{i_l}$ denotes the summation over all of the samples in the $l$-th leaves.\n",
    "\n",
    "Then $Q_l$ that minimizes $Loss$ is \n",
    "$$\n",
    "Q_{ml} = -\\frac{g_l}{h_l + \\lambda}\n",
    "$$\n",
    "And we have the minimum value of $\\delta L $ as  \n",
    "$$\n",
    "  \\delta L_m =  - \\frac{1 \\over 2}\\sum_l \\frac{g_l^2} {h_l + \\lambda}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Regression\n",
    "For regression, we use\n",
    "$$\n",
    "  L(y, x) = {1\\over 2} (y - x)^2.\n",
    "$$\n",
    "Then we have \n",
    "$$\n",
    "  g_l = -\\sum_{i_l}(y_{i_l} - p^{k-1}_{i_l}) = -\\sum_{i_l}r_{i_l}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "  h_l = n_l + \\lambda\n",
    "$$\n",
    "where $n_l$ is the number of the samples in the $l$-th leaf. Then we \n",
    "$$\n",
    "  \\delta L_m  = -{1 \\over 2}\\sum_l S_l\n",
    "$$\n",
    "with\n",
    "$$\n",
    "  S_l = \\frac{(\\sum_{i_l}r_{i_l})^2} {n_l + \\lambda}\n",
    "$$\n",
    "This is just the Similarity Score of the $l$-th leaf.\n",
    "Addtionally, we have \n",
    "$$\n",
    "Q_{ml} = -\\frac{g_l}{h_l + \\lambda} = \\frac{\\sum_{i_l}r_{i_l}}{n_l + \\lambda}.\n",
    "$$\n",
    "This is the output of the $l$-th leaf.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Binary classification\n",
    "\n",
    "For binary classification, we have\n",
    "$$\n",
    "  L(y, x) = - [y\\ln x + (1-y) \\ln (1-x)] = -y \\ln \\frac{x}{1-x} - \\ln (1-x) = -yz + \\ln(1+e^z)\n",
    "$$\n",
    "with \n",
    "$$\n",
    "  z \\equiv \\ln\\frac{x} {1-x}.\n",
    "$$\n",
    "Then we can rewrite $L(y,x)$ as the function $\\tilde{L}(y,z)$\n",
    "$$\n",
    "  \\tilde{L}(y,z) = -yz + \\ln(1+e^z)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "  Loss = \\sum_{i=1}^m \\tilde{L}(y_i, z^{k-1}_i + Q_i) \\simeq \\sum_{i=1}^m \\tilde{L}(y_i, z^{k-1}_i) + \\delta L\n",
    "$$\n",
    "with \n",
    "$$\n",
    "  \\delta L = \\sum_l [g_l Q_l + {1\\over2}(h_l+\\lambda) Q_l^2].\n",
    "$$\n",
    "and\n",
    "$$ \n",
    "  g_l = \\sum_{i_l} \\frac{\\partial \\tilde{L}(y, z)}{\\partial z}\\bigg |_{y=y_{i_l}, z=z^{k-1}_{i_l}}\n",
    "$$\n",
    "$$\n",
    " h_l = \\sum_{i_l} \\frac{\\partial^2 \\tilde{L}(y, z)}{\\partial x^2}\\bigg |_{y=y_{i_l}, z=p^{k-1}_{i_l}}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have \n",
    "$$\n",
    " g_l = \\sum_{i_l} \\left(-y_{i_l} + \\frac{e^{z_{i_l}^{k-1}}}{1 + e^{z_{i_l}^{k-1}}} \\right)\n",
    "     = - \\sum_{i_l} (y_{i_l} - p_i^{k-1}) = -\\sum_{i_l} r_{i_l} \n",
    "$$\n",
    "and \n",
    "$$\n",
    "  h_l = \\sum_{i_l} \\frac{e^{z_{i_l}^{k-1}}} {\\left(1 + e^{z_{i_l}^{k-1}}\\right)^2} = \\sum_{i_l}p^{k-1}_{i_l} (1 - p^{k-1}_{i_l})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Features of XGBoost\n",
    "### Parallelization\n",
    "XGBoost parallelizes the training process, not at tree-level but at feature-level.\n",
    "\n",
    "### Approximate Split and Weighted Quntile Sketch with Hessian\n",
    "For large dataset, sketch_eps (default at 0.03 in xgboost) to control the granularity of the buckets.\n",
    "This means, defaultly, approximate 33 percentiles are used for the approximate split algorithm.\n",
    "The percentiles are determined based on the weights. This is called Weighted Quntile Sketch. The weight with Hessian\n",
    "\n",
    "### Random Forest via XGBoost\n",
    "Use RF as a base learner in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have \n",
    "$$\n",
    "  \\delta L_m = -{1\\over 2} \\sum_{l} S_l\n",
    "$$\n",
    "with \n",
    "$$\n",
    "   S_l = \\frac{ (\\sum_{i=1}^n r_i)^2} {\\lambda + \\sum_{i_l} p^{k-1}_{i_l}(1-p^{k-1}_{i_l})}.\n",
    "$$\n",
    "This is just the Similarity Score for binary classification.\n",
    "Additionally, we have\n",
    "$$\n",
    "  Q_{ml} = -\\frac{g_l}{h_l + \\lambda} = \\frac{ \\sum_{i_l} r_{i_l}}{\\lambda + \\sum_{i_l} p^{k-1}_{i_l}(1-p^{k-1}_{i_l})}.\n",
    "$$\n",
    "This is the output of a leaf. Then by combining the initial ouput and the output of all of the XGBoost tress,\n",
    "we have the output of the model as \n",
    "$$\n",
    "Q_T = \\ln \\frac{p_0}{1-p0} + \\epsilon \\sum_t {Q_mt}\n",
    "$$\n",
    "However, as displayed above, this is $z^{k}$, not the prediction.\n",
    "The prediction $p$ is \n",
    "$$\n",
    " Q_T = \\ln \\frac{p}{1-p} \\Rightarrow p = \\frac{1} {1 + e^{-Q_T}}.\n",
    "$$\n",
    "We get the result displayed in Sec. 1.2.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
