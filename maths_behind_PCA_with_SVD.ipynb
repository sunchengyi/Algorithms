{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maths_behind_PCA_with_SVD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZTfYk1YjMTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQFFOZrsH11P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(4)\n",
        "m = 60\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "X = np.empty((m, 3))\n",
        "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYpR3pOsJPff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAUeQo-qJ2B9",
        "colab_type": "text"
      },
      "source": [
        "# PCA using Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esWVmIDzJ897",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XPuE-jyKYDw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "36cd8e5d-1504-43ac-bcae-0975f450dc47"
      },
      "source": [
        "X2D[:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.26203346,  0.42067648],\n",
              "       [-0.08001485, -0.35272239],\n",
              "       [ 1.17545763,  0.36085729],\n",
              "       [ 0.89305601, -0.30862856],\n",
              "       [ 0.73016287, -0.25404049]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBLAm1AzKU8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f586fbb-08d6-4327-d84b-aee52a2e8aa4"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t__5eTbfcqAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI5O3bA6Kl8Z",
        "colab_type": "text"
      },
      "source": [
        "It is trivial to do PCA using Scikit-Learn. But what is the algorithm behind the result? It can be explained by the sigular value decomposition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWuv1kneN1wq",
        "colab_type": "text"
      },
      "source": [
        "# PCA using Numpy with SVD (sigular value decompsition)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oCd_INxIoQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, V = np.linalg.svd(X_centered)\n",
        "w2 = V.T[:, :2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSvdSpTeO7us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X2D_svd = X_centered.dot(w2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R45xZWE6PIq_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bbe5da66-0494-4ee7-8903-c7a4218f095b"
      },
      "source": [
        "X2D_svd[:5]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.26203346, -0.42067648],\n",
              "       [ 0.08001485,  0.35272239],\n",
              "       [-1.17545763, -0.36085729],\n",
              "       [-0.89305601,  0.30862856],\n",
              "       [-0.73016287,  0.25404049]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRG0p22gPmMn",
        "colab_type": "text"
      },
      "source": [
        "So we get the same result except with the opposite sign. But this brings even more questions. Why should the data be centred? Why can SVD implement PCA?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EBWUm4tRzl1",
        "colab_type": "text"
      },
      "source": [
        "# Mathematics Behind PCA with SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_nJes-uSTDF",
        "colab_type": "text"
      },
      "source": [
        "PCA is to find the axis that account for the largest amount of variance for the training set. In the above example, it is to find such an axis for the set, X. The variance of X along its first axis (column) is var(X[:,0]). \n",
        "\n",
        "The sum of all of the variance along each axis (column) are:\n",
        "    \n",
        "    var_total = np.trace(X_centred.T @ X_centered) = np.trace(X_centred @ X_centered.T).\n",
        "Denoting the eigenvalues of (X_centred.T @ X_centered) as $\\lambda_i$, then we also have\n",
        "\n",
        "var_total = $\\sum_i\\lambda_i$\n",
        "\n",
        "And the variance along an arbitrary axis can be expressed as\n",
        "\n",
        "var = $\\sum_i\\alpha_i\\lambda_i$ \n",
        "\n",
        "under the condition \n",
        "\n",
        "$\\sum_i\\alpha_i=1$.\n",
        "\n",
        "So the largest variance can be achieived by along the eigenvector of the largest eigenvalue.\n",
        "\n",
        "By reading the documents about np.linalg.svd, we know that \"s\" returned by np.linalg.svd(X_cnetered) is the vector of eigenvalues of (X_centred.T @ X_centered) or (X_centred.T @ X_centered) in the descending order. The rows in \"V\" are the corresponding eigenvectors. So \n",
        "\n",
        "X_centered.dot(V.T[:, :2]) \n",
        "\n",
        "is what we want for PCA with the largest two variances!"
      ]
    }
  ]
}