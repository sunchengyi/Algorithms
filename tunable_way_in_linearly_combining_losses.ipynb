{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I read a vert interesting blogs, [How we can make machine learning algorithm tunable](https://engraved.ghost.io/how-we-can-make-machine-learning-algorithms-tunable/), post Engraved by J. Degrave and I. Korshunova. \n",
    "Here, I summary the result and give my understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solve-The-Dual method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(θ, λ, ε):\n",
    "  return loss_1(θ) - λ*(ε - loss_2(θ))\n",
    "\n",
    "loss_derivative = grad(loss)\n",
    "ε = 0.3 \n",
    "λ = solve_dual(ε)  # The crux\n",
    "\n",
    "for gradient_step in range(200):\n",
    "  gradient = loss_derivative(θ, λ, ε)\n",
    "  θ = θ - 0.02 * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work, since this is just the origin linear trade-off between two losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hard Constraint First Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint(θ, ε):\n",
    "  return ε - loss_2(θ)\n",
    "\n",
    "optimization_derivative = grad(loss_1)\n",
    "constraint_derivative = grad(constraint)\n",
    "\n",
    "ε = 0.7\n",
    "\n",
    "for gradient_step in range(200):\n",
    "  while constraint(θ, ε) < 0:\n",
    "    # maximize until the constraint is positive again\n",
    "    gradient = constraint_derivative(θ, ε)\n",
    "    θ = θ + 0.02 * gradient\n",
    "    \n",
    "  gradient = optimization_derivative(θ)\n",
    "  θ = θ - 0.02 * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration of this method, firstly, $\\theta$ is changed iteratively to make the constraint condition being\n",
    "satisfied. So, actually, loss_1 will not effect until the constraint condition is satisfied.\n",
    "\n",
    "The authors said:  \n",
    "\"Additionally, this method does not work that well when you want to use stochastic gradient descent rather than gradient descent. Since the constraint is defined on the average loss across all data, you do not want to enforce the hard constraint on a sample of your data where it is not satisfied yet, as long as it is satisfied in the general case. And this issue is hard to overcome.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Differential Multiplier Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrangian(θ, λ, ε):\n",
    " return loss_1(θ) - λ*(ε - loss_2(θ))\n",
    "\n",
    "derivative = grad(lagrangian, (0,1))\n",
    "ε = 0.7\n",
    "λ = 0.0\n",
    "\n",
    "for gradient_step in range(200):\n",
    "  gradient_θ, gradient_λ = derivative(θ,λ,ε)\n",
    "  θ = θ - 0.02 * gradient_θ  # Gradient descent\n",
    "  λ = λ + gradient_λ  # Gradient ascent!\n",
    "  if λ < 0:\n",
    "    λ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Differential Method of Multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrangian(θ, λ, ε):\n",
    " damp = 10 * stop_gradient(ε-loss_2(θ))\n",
    " return loss_1(θ) - (λ-damp) * (ε-loss_2(θ))\n",
    "\n",
    "derivative = grad(lagrangian, (0,1))\n",
    "ε = 0.7\n",
    "λ = 0.0\n",
    "\n",
    "for gradient_step in range(200):\n",
    "  gradient_θ, gradient_λ = derivative(θ, λ, ε)\n",
    "  θ = θ - 0.02 * gradient_θ\n",
    "  λ = λ + gradient_λ\n",
    "  if λ < 0:\n",
    "    λ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
